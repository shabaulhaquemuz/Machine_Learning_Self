{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb290736-555f-4f2a-bacd-127ae4c0ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine is a supervised machine learning algorithm which is used for both classfication and regression task\n",
    "#it is particularly effective in dealing with complex and high-dimensional dataset\n",
    "#the fundamental principle of SVM is to find an optimal hyperplane that maximumly separates different classes in the input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f998fd-71fe-4003-83c3-31f56ecff72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How SVM works:\n",
    "#1. SVM requires labelled training data consisting of input features and corresponding class labels\n",
    "#2. Each data point is represented as feature vector where each feature describes a particular characteristics of the data points\n",
    "#3. Then the data point should be preprocessed and scaled to ensure the feature are on similar scale typically between 0 and 1\n",
    "#4. Hyperplane and Margin: SVM aims to find a hyperplane that best separates the different classes in the feature space. In a binary classification problem,\n",
    "# the hyperplane is a line in 2d space, or a hyperplane in a higher dimensional space.\n",
    "#5. SVM seeks to maximize the margin which is the distance betweeen the hyperplane and the nearest data points from each class. The points on the margin\n",
    "# are known as support vectors as they play a crucial role  in defining the decision boundary.\n",
    "\n",
    "#Linear Kernell: The linear SVM finds a linear hyperplane that separates the classes. The goal is to find the hyperplane that maximize the margin while \n",
    "# minimizing the missed classification of training examples. This can be formulated as an optimization problem with the objective of minimizing the weights\n",
    "# of the hyperplane subject to the constraint that all training examples lie on the correct side of the hyperplane.\n",
    "\n",
    "#Non-Linear SVM: IN cases where the data is non-linearly separable, SVM uses a technique called the Kernel-trick. The Kernel-trick maps the original input\n",
    "#space into a higher dimensional space where the data points can be linearly separable.\n",
    "\n",
    "#Training: SVM training involves finding the optimum hyperplane for decision boundary that separates the classes. The optimization problem is typically\n",
    "# solved using methods such as quadratic programming or sequential minimal optimization. The process involves solving for the weights of the hyperplane and\n",
    "# the biased term which defines the decision the decision boundary. The objective is to minimize the regularization terms while ensuring that the training examples\n",
    "# correctly classified.\n",
    "\n",
    "#Prediction: Once the SVM model is trained, it can be used to predict the class label of new unseen data points. This algorithm computes the distance from\n",
    "# the test points to the decision boundary and the predicted class label is determined based on width size of the decision boundary the point lies.\n",
    "# The decision function can also provide a confidence score and the probability estimates for the prediction.\n",
    "\n",
    "#Advantage of SVM: \n",
    "#1. Effective for high-dimensional space\n",
    "#2. it is robust against overfitting due to the marigin-maximization principle\n",
    "#3. it is versatile through the use of different kernel-function \n",
    "#4. it can hanlde both linear and non-linear classification task\n",
    "\n",
    "#Limitations:\n",
    "#1. it is Computationally expensive for large datsets\n",
    "#2.  it requires proper selection of hyper parameters such as the regularization paramaeter and the kernel parameter\n",
    "#3. it is difficult to interpret the large model compared to simpler algorithm like logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663131a3-5610-4d39-a382-cdd15a9dbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How i select which kernel i have to use for which data in SVM algorithm?\n",
    "#the choice of kernel depends on the data we arevtrying to solve\n",
    "\n",
    "\n",
    "#1. Linear Kernal: it is suitable for linearly separable data. it works well when there is a clear linear boundary between classes\n",
    "#2. Polynomial Lernel: It maps the data into a higher dimensional space using polynomial functions. it is useful when  the decision boundary is curved or\n",
    "# has higher degree complexity. The degree of the polynomial which determines the complexity can be specified \n",
    "#3. RBF kernel(Gaussian-Kernel): The gaussian-kernel max the data into an infinity dimensional space. it is suitable for non-linearly separable data and \n",
    "# works well when the decision boundary is complex. it is a popular choice due tpo its flexibility and ability to capture instigates patterns\n",
    "\n",
    "#4. Sigmoid Kernel: this kernel maps the data into a higher dimensional space using the sigmoid function. it is useful when the decision  boundary is S shaped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e6f388-122a-44c2-8b43-e6f3c794102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7916378-db4c-49d0-990a-64f711babee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate synthetic classification data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                          n_redundant=0, random_state=42)\n",
    "\n",
    "#make_classification: This uses make_classification from sklearn.datasets, which is a synthetic dataset generator.\n",
    "# Purpose of make_classification\n",
    "# To quickly create artificial datasets for testing machine learning algorithms.\n",
    "# Instead of using a real-world dataset (which may be large, noisy, or hard to get), you can generate clean, controlled data.\n",
    "# Great for demonstrations, debugging, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "332fdfe0-1b7e-4627-9eb1-4e189fe9e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88c969e-f752-4fe7-948b-093dab85fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Linear kernel\n",
    "linear_svc=svm.SVC(kernel=\"linear\")\n",
    "linear_svc.fit(X_train,y_train)\n",
    "linear_predictions=linear_svc.predict(X_test)\n",
    "linear_accuracy=accuracy_score(y_test,linear_predictions)\n",
    "print(\"linear kernel accuracy:\",linear_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461339a2-68fe-4036-9409-3248a33d1a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomial kernel accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "poly_svc=svm.SVC(kernel=\"poly\",degree=3)  #degree=3 ==> x**0 + x**1 + x**2 ==> 1+x+x**2\n",
    "poly_svc.fit(X_train,y_train)\n",
    "poly_predictions=poly_svc.predict(X_test)\n",
    "poly_accuracy=accuracy_score(y_test,poly_predictions)\n",
    "print(\"polynomial kernel accuracy:\",poly_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f238c8f-c110-480b-9936-72e88782f51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
